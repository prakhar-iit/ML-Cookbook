{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1acb39",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "Different Steps in traditional NLP Pipeline are:\n",
    "1. **Text Cleaning -** Cleaning of HTML Tags and other markdowns from text data\n",
    "2. **Text Preprocessing -** Noise Removal, Tokenization of words and phrases, Standardization and Normalization of words, objects and tokens.\n",
    "3. **Feature Engineering -** POS Tagging of words in a sentence, Syntactic relationship of different words in a sentence, Recognition of entities in a sentence (NER) - Identification and classification of Entities.\n",
    "    1. Rule Based Feature extraction \n",
    "    2. Statistical Features\n",
    "    3. Word Embeddings\n",
    "4. **NLP Tasks -**\n",
    "    1. Sentiment Classification\n",
    "    2. Autocorrect & Autocomplete in Search bar\n",
    "    3. Social Media Monitoring\n",
    "    4. Chatbots\n",
    "    5. Text Summarization\n",
    "    6. Machine Translation\n",
    "    7. NLG & NLU\n",
    "    8. Document to Information\n",
    "    9. Topic Modeling\n",
    "    10. Curated content feed that has highest information entropy\n",
    "    \n",
    "Word2Vec is a vector representation of a word in higher dimensions. It is used to represent the word semantically. It starts from One hot encoding of vector in which size of vector is equal to the size of the vocabulary.\n",
    "\n",
    "Objective of traditional NLP is to model the language using rules\n",
    "Traditional NLP models are rule based and good for small and specific tasks. They don't require a lot of compute compared to the Gen AI Models. But the speed and operating cost of these models comes at a cost of lower accuracy and less flexibility of the models.\n",
    "\n",
    "Gen AI Models are more generic and gives more flexibility to the user who uses these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea67c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8eea",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "- Noise Removal \n",
    "    - Stopword removal like is, am, or, of, an \n",
    "    - Punctuation removal\n",
    "    - Removing URLs, Links, Social Media hashtags, mentions\n",
    "    - lower casing\n",
    "- Tokenization\n",
    "- Lexicon Normalization \n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "- Object Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb1d90",
   "metadata": {},
   "source": [
    "## 2.a) Noise Removal\n",
    "- List based removal or regex based approach to remove specific words that are not providing lot of useful information\n",
    "- Various Other Noise Removal Steps:\n",
    "   - Encoding-Decoding Noise\n",
    "   - Grammar Check\n",
    "   - Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bce755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_list = [\"is\", 'a', \"this\"]\n",
    "def remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if(word not in noise_list)]\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03dc27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text\n"
     ]
    }
   ],
   "source": [
    "print(remove_noise(\"this is a sample text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af805a14",
   "metadata": {},
   "source": [
    "## 2.b) Lexicon Normalization\n",
    "- **Stemming** \n",
    "    - Rule based process to strip the suffixes - Reduce word to their root or ase form\n",
    "    - Text Standardization\n",
    "    - Can cause the root form of word to loose its meaning \n",
    "- **Lemmatization** \n",
    "    - Step by step process of obtaining the root form of the word. Use of vocab & morphological analysis\n",
    "    - Restores the words into the root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597ba072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee860fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prakharrastogi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b45595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gees\n",
      "gee\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "word = \"geese\"\n",
    "word = stem.stem(word)\n",
    "print(word)\n",
    "print(lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81711b",
   "metadata": {},
   "source": [
    "## 2.c) Object Standardization\n",
    "- Words are not recognized by search enines and models\n",
    "- Acronyms, hastags, colloquial slangs\n",
    "- Using Regex, and Manually prepared data dictionaries - this type of noise can be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7bd3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm': 'direct message', \"awsm\": \"awesome\", \"luv\": \"love\"}\n",
    "def word_standardize(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "    new_text = \" \".join(new_words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a713684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet this is a retweeted tweet by Prakhar Rastogi, direct message Me\n"
     ]
    }
   ],
   "source": [
    "print(word_standardize(\"RT this is a retweeted tweet by Prakhar Rastogi, DM Me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98366630",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering on Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae50a6a",
   "metadata": {},
   "source": [
    "## 3.a Syntactic Parsing\n",
    "Analysis of Words in sentence for grammar and their arrangement that shows relationships among words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c0a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e92335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prakharrastogi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/prakharrastogi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe1d89",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "### Applications of POS Tagging\n",
    "- Word Sense Diambiguation\n",
    "- Improving Word based features\n",
    "- Normalization and Lemmatization\n",
    "- Efficient Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3cb1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "872d65c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2433f8",
   "metadata": {},
   "source": [
    "## 3.b Entity Extraction\n",
    "- Entity detetction algorithms are ensemble models of **rule based parsing**, dictionary lookups, pos tagging, dependency parsing\n",
    "\n",
    "## NER\n",
    "NER model consists of 3 blocks: \n",
    "- Noun Phrase Identification\n",
    "- Phrase Classification\n",
    "- Entity Disambiguation\n",
    "\n",
    "- Spacy library is there to perform basic NER. \n",
    "- Special models can be trained for use-case specific NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de17059",
   "metadata": {},
   "source": [
    "# 4. Topic Modeling\n",
    "- A repeating pattern of co-occuring terms in a corpus is called Topic.\n",
    "- Identifying topics present in text corpus and derives hidden patterns among words in corpus in unsupervised manner.\n",
    "- Essential Statistical tool to reveal abstract topics present in set of documents.\n",
    "- Enabling discovery of concealed semantic structures within a body of text.\n",
    "- Gain insight into **underlying themes and concepts embedded in documents** under investigation.\n",
    "- Rule-based text mining - regex or keyword search based\n",
    "- Useful for **document clustering, organizing blocks of textual data, categorization and labeling of articles on internet, information retrieval from unstructured data and feature selection**\n",
    "\n",
    "- **Applications**\n",
    "    - Text Content Recommendation Engines\n",
    "    - Job Recommendation to right candidate\n",
    "    - Organize emails, customer reviews, & user social media profiles\n",
    "    - Extracting Meaningful entities from 10-K filings or other financial texts\n",
    "    - Extract entities from legal documents\n",
    "    - Detecting breaking news on social media\n",
    "    - Detect Fake messages\n",
    "    - Recommend Personalized Messages\n",
    "    - Characterizing Information Flow\n",
    "    \n",
    "- **Downstream Usecases**\n",
    "    - Topic Analysis over the period of time (or Trend Analysis)\n",
    "    - Combine topical data with geographic and demographic data like zip code, income range of zip codes, etc.\n",
    "\n",
    "- **Statistical Techniques**\n",
    "    - Term Frequency\n",
    "    - Inverse Document Frequency\n",
    "    - Non-negative Matrix Factorization\n",
    "    - LDA (Most popular)\n",
    "    \n",
    "## Latent Dirichlet Allocation\n",
    "- Document Term Matrix formation i,j represents frequency count of word Wj in Document Di (N x M)\n",
    "- LDA converts Document term matrix into lower dimension matrix - M1 and M2\n",
    "- M1 - Document - Topic Matrix (N x K)\n",
    "- M2 - Topic - Term Matrix (K x M)\n",
    "- Parameters of LDA\n",
    "    - Alpha - Document-Topic density\n",
    "    - Beta - Topic-word density\n",
    "    - High Alpha - More topics and vice versa\n",
    "    - High beta - Lareg number of words and vice-versa\n",
    "    \n",
    "- Perform Preprocessing steps for better performance\n",
    "\n",
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236a6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prakharrastogi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad2cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "doc_complete = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d7951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = \"Unless otherwise set forth in Policies (as defined in section 2.10), or required by state or federal law, Facility shall submit Claims to Plan, subject to any applicable HIPAA requirements, using appropriate and current Coded Service Identifiers, within one hundred twenty (120) days from the date the Health Services are rendered or Plan will refuse payment. If Plan is the secondary payor,the one hundred twenty (120) day period will not begin until Facility receives notification of primary payor's responsibility.\"\n",
    "doc_complete = [doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f44d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n",
    "    punc_free = ''.join([character for character in stop_free if character not in exclude])\n",
    "    normalized = \" \".join(lem.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc821dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['unless', 'otherwise', 'set', 'forth', 'policy', 'a', 'defined', 'section', '210', 'required', 'state', 'federal', 'law', 'facility', 'shall', 'submit', 'claim', 'plan', 'subject', 'applicable', 'hipaa', 'requirement', 'using', 'appropriate', 'current', 'coded', 'service', 'identifier', 'within', 'one', 'hundred', 'twenty', '120', 'day', 'date', 'health', 'service', 'rendered', 'plan', 'refuse', 'payment', 'plan', 'secondary', 'payorthe', 'one', 'hundred', 'twenty', '120', 'day', 'period', 'begin', 'facility', 'receives', 'notification', 'primary', 'payors', 'responsibility']]\n"
     ]
    }
   ],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "print(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c73754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(doc_clean):\n",
    "    # Creating term dictionary of our corpus, where every unique term is assigned an index\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    #Converting list of documents into Document term matrix using dictionary\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    #Creating object for LDA model using gensim library\n",
    "    lda = gensim.models.ldamodel.LdaModel\n",
    "    #Running and training LDA model on document term matrix\n",
    "    ldamodel = lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e59443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.046*\"plan\" + 0.032*\"facility\" + 0.032*\"twenty\"'),\n",
       " (1, '0.021*\"date\" + 0.021*\"rendered\" + 0.021*\"otherwise\"'),\n",
       " (2, '0.021*\"secondary\" + 0.021*\"forth\" + 0.021*\"refuse\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel = topic_model(doc_clean)\n",
    "ldamodel.print_topics(num_topics=3, num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d900b",
   "metadata": {},
   "source": [
    "## Tips to improve Topic Modeling results\n",
    "- **Frequency Filter:** Remove low frequency terms as they are weak features in document term matrix. Perform exploratory analysis to determine critical frequency value\n",
    "- **POS Tag Filter**\n",
    "- **Batch-wise LDA:** Run LDA multiple times will provide different results and best topic terms will be the intersection of all batches\n",
    "\n",
    "## Problems with Unsupervised Topic Model\n",
    "- Overlapping and Correlated Topics\n",
    "- **Bag of words:** Words are exchangeable and sentence is not modeled\n",
    "- **Unsupervised:** Weak supervision is desirable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf3a91",
   "metadata": {},
   "source": [
    "# 5. Statistical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c2f36",
   "metadata": {},
   "source": [
    "## N-grams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b0d92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []\n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1581d133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2ce57",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF - IDF)\n",
    "- Weighted Model used for Information Retrieval Problems.\n",
    "- Convert text into vector models on the basis of occurence of words in documents without considering exact ordering\n",
    "- **Term** = Count of a term\n",
    "- **IDF** = Log of ratio os total docs available in corpus and number of documents containing term 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73b75119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b81f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i.\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "001a68ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34520501686496574\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 7)\t0.5844829010200651\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 4)\t0.444514311537431\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efeb89",
   "metadata": {},
   "source": [
    "## Count/Density/Readability Features\n",
    "- Used in models and analysis\n",
    "- Features that are trivial but shows great impact in learning models\n",
    "- Features - Word Count, Sentence Count, Punctuation Count, Industry Specific Word Count\n",
    "- **Other Measures -** Syllable Counts, Smog Index, Flesch Reading Ease\n",
    "- **SMOG Index -** \"Simple Measure of Gobbledygook\" is a readability formula that estimates how many years of education a person needs to understand a piece of writing. Mostly useful for texts with at least 30 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6df1e0",
   "metadata": {},
   "source": [
    "# Word Embedding (Text Vectors)\n",
    "- Modern Way of representing words as vectors.\n",
    "- Redefine high dim word features into low dim features by preserving contextual similarity in corpus\n",
    "- Word2Vec & GloVe - two text embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a058a",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f01a2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.023671655\n"
     ]
    }
   ],
   "source": [
    "#Combination of preprocessing model, 2 shallow neural network (Bag of Words & skip-gram)\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model.wv.similarity('data', 'science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a07982",
   "metadata": {},
   "source": [
    "# 6. Important NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1046f7",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "- Systematically classify a text object in one of the fixed category to organize, and filter information and storage purposes\n",
    "- Email Spam Identification\n",
    "- Topic Classification of news\n",
    "- Sentiment Classification\n",
    "- Organization of web pages by search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec5e3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "55da8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = [('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "\n",
    "test_corpus = [(\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "677f2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NBC(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b385163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n"
     ]
    }
   ],
   "source": [
    "print(model.classify(\"Their codes are amazing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "698f0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_B\n"
     ]
    }
   ],
   "source": [
    "print(model.classify(\"I don't like their computer.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ed28ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba5be7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "43f934ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for SVM model\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "    \n",
    "test_data = []\n",
    "test_labels = []\n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "afb69dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_A', 'Class_A', 'Class_B', 'Class_B', 'Class_A', 'Class_A'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "225190de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class_A       0.50      0.67      0.57         3\n",
      "     Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad96e81",
   "metadata": {},
   "source": [
    "# Text Matching/Simialrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed386c5f",
   "metadata": {},
   "source": [
    "- Matching of text objects and find similarities\n",
    "- Applications: \n",
    "    - Automatic Spelling Correction\n",
    "    - Date De-duplication\n",
    "    - Genome Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e6f68",
   "metadata": {},
   "source": [
    "## Levenshtein Distance \n",
    "- Min number of edits needed to transform one string to other, with allowable edit operations being Insertion, Deletion, Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "54b6dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a8192f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"analyze\", \"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6206fb3b",
   "metadata": {},
   "source": [
    "## Phonetic Matching\n",
    "- Produces a character string that identifies set of words that are roughly phonetically similar.\n",
    "- Very useful for searching large text corpuses, correcting spelling errors, and matching relevant names\n",
    "- **Soundex** and **Metaphone** are two main phonetic algorithms used for purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4351968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fda25ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex = fuzzy.Soundex(4)\n",
    "soundex(\"ankit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0ca22",
   "metadata": {},
   "source": [
    "## Flexible String Matching\n",
    "- Exact String Matching\n",
    "- Lemmatized Matching\n",
    "- Compact Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4f3b",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "- Also known as vector simialarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ee8c0",
   "metadata": {},
   "source": [
    "## Coreference Resolution\n",
    "- Process of finding relational links among words within sentences\n",
    "- Component of NLP that does this job automatically\n",
    "- Used in document summarization, question answering, and information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c72d2d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "- https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c545e",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "- Using BERT and Bayes for automating data extraction tasks from diverse sources including medical records\n",
    "- POS: 8 POS in english language - noun, pronoun, verb, adjective, adverb, preposition, conjuction, and intersection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef542cc",
   "metadata": {},
   "source": [
    "## Opinion Mining\n",
    "- Classify yelp review into one of five aspects: Food, service, price, ambience, or anecdotal/miscellaneous\n",
    "- Use Kaggle labelled data for opinion mining using LLM\n",
    "- In a pipeline, we can use topic modeling before aspect based model\n",
    "- Use **Aspect-Based Model** - Requires features/topic to look at. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ee402",
   "metadata": {},
   "source": [
    "## Knowledge Graphs\n",
    "- Sentence Segmentation: Split the text document or article into sentences\n",
    "- https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2015/04/pagerank-explained-simple/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
